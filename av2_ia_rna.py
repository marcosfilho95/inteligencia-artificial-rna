# -*- coding: utf-8 -*-
"""AV2 IA - RNA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-1v58FoQ2RI0QkJDUXVgYPmXIgXhjky3

#üî∑ ETAPA 1 ‚Äì REGRESS√ÉO
üé≤ Dados: aerogerador.dat
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install numpy matplotlib seaborn

import numpy as np
data = np.loadtxt('/content/aerogerador.dat')

import numpy as np
import matplotlib.pyplot as plt

data = np.loadtxt("aerogerador.dat")
X = data[:, 0].reshape(1, -1)
Y = data[:, 1].reshape(1, -1)

# Min-max [-1, 1]
X_min, X_max = X.min(), X.max()
Y_min, Y_max = Y.min(), Y.max()

X_norm = 2 * (X - X_min) / (X_max - X_min) - 1
Y_norm = 2 * (Y - Y_min) / (Y_max - Y_min) - 1

"""Os dados foram normalizados no intervalo [-1, +1] com a t√©cnica Min-Max, ideal para redes com fun√ß√£o de ativa√ß√£o tanh."""

plt.figure(figsize=(8, 5))
plt.scatter(X_norm.flatten(), Y_norm.flatten(), c='teal', s=20, edgecolors='k',linewidth=0.4, alpha=0.7)
plt.title("Velocidade do Vento vs Pot√™ncia Gerada (Normalizados)")
plt.xlabel("Velocidade (normalizada)")
plt.ylabel("Pot√™ncia (normalizada)")
plt.grid(True)
plt.show()

"""Aqui est√° o gr√°fico de dispers√£o dos dados normalizados. Ele mostra a rela√ß√£o entre a velocidade do vento (entrada) e a pot√™ncia gerada (sa√≠da) pelo aerogerador."""

class Adaline:
    def __init__(self, input_dim, learning_rate=0.01, epochs=500):
        self.weights = np.random.randn(1, input_dim) * 0.01
        self.bias = np.zeros((1, 1))
        self.lr = learning_rate
        self.epochs = epochs
        self.loss_history = []

    def predict(self, X):
        return np.dot(self.weights, X) + self.bias

    def fit(self, X, Y):
        N = X.shape[1]
        for _ in range(self.epochs):
            Y_pred = self.predict(X)
            error = Y - Y_pred
            mse = np.mean(error ** 2)
            self.loss_history.append(mse)
            self.weights -= self.lr * (-2 / N) * np.dot(error, X.T)
            self.bias -= self.lr * (-2 / N) * np.sum(error)

    def score(self, X, Y):
        return np.mean((Y - self.predict(X)) ** 2)

adaline = Adaline(input_dim=1, learning_rate=0.01, epochs=500)
adaline.fit(X_norm, Y_norm)

plt.plot(adaline.loss_history)
plt.title("Curva de Aprendizado - ADALINE")
plt.xlabel("√âpocas")
plt.ylabel("Erro (MSE)")
plt.grid(True)
plt.show()

print("MSE final (treino):", adaline.score(X_norm, Y_norm))

class MLP:
    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.001, epochs=500):
        self.W1 = np.random.randn(hidden_dim, input_dim) * 0.01
        self.b1 = np.zeros((hidden_dim, 1))
        self.W2 = np.random.randn(output_dim, hidden_dim) * 0.01
        self.b2 = np.zeros((output_dim, 1))
        self.lr = learning_rate
        self.epochs = epochs
        self.loss_history = []       # Erro de treino por √©poca
        self.test_loss_history = []  # Erro de teste por √©poca

    def tanh(self, z):
        return np.tanh(z)

    def tanh_derivative(self, z):
        return 1 - np.tanh(z) ** 2

    def predict(self, X):
        Z1 = np.dot(self.W1, X) + self.b1
        A1 = self.tanh(Z1)
        Z2 = np.dot(self.W2, A1) + self.b2
        return Z2

    def fit(self, X, Y, X_test=None, Y_test=None):
        N = X.shape[1]
        for _ in range(self.epochs):
            # Forward
            Z1 = np.dot(self.W1, X) + self.b1
            A1 = self.tanh(Z1)
            Z2 = np.dot(self.W2, A1) + self.b2
            Y_pred = Z2

            # Erro de treino
            error = Y_pred - Y
            mse = np.mean(error ** 2)
            self.loss_history.append(mse)

            # Erro de teste (se fornecido)
            if X_test is not None and Y_test is not None:
                Y_test_pred = self.predict(X_test)
                test_mse = np.mean((Y_test_pred - Y_test) ** 2)
                self.test_loss_history.append(test_mse)

            # Backpropagation
            dZ2 = error
            dW2 = (1 / N) * np.dot(dZ2, A1.T)
            db2 = (1 / N) * np.sum(dZ2, axis=1, keepdims=True)

            dA1 = np.dot(self.W2.T, dZ2)
            dZ1 = dA1 * self.tanh_derivative(Z1)
            dW1 = (1 / N) * np.dot(dZ1, X.T)
            db1 = (1 / N) * np.sum(dZ1, axis=1, keepdims=True)

            # Atualiza√ß√£o dos pesos
            self.W1 -= self.lr * dW1
            self.b1 -= self.lr * db1
            self.W2 -= self.lr * dW2
            self.b2 -= self.lr * db2

    def score(self, X, Y):
        Y_pred = self.predict(X)
        return np.mean((Y_pred - Y) ** 2)

# Topologia: 5 neur√¥nios na camada oculta
mlp = MLP(input_dim=1, hidden_dim=5, output_dim=1, learning_rate=0.01, epochs=500)
mlp.fit(X_norm, Y_norm)

plt.plot(mlp.loss_history)
plt.title("Curva de Aprendizado - MLP (5 neur√¥nios)")
plt.xlabel("√âpocas")
plt.ylabel("Erro (MSE)")
plt.grid(True)
plt.show()

print("MSE final (treino):", mlp.score(X_norm, Y_norm))

"""A curva de aprendizado acima ilustra a redu√ß√£o do Erro Quadr√°tico M√©dio (MSE) ao longo das √©pocas de treinamento do modelo MLP com 5 neur√¥nios na camada oculta. Observa-se uma tend√™ncia de converg√™ncia, indicando que o modelo est√° conseguindo aprender de forma progressiva a rela√ß√£o entre as vari√°veis de entrada e sa√≠da. O decr√©scimo do erro ao longo das itera√ß√µes sugere que o ajuste dos pesos est√° sendo eficaz, com o modelo aproximando-se de uma configura√ß√£o est√°vel."""

def split_once(X, Y):
    N = X.shape[1]
    idx = np.random.permutation(N)
    train_idx = idx[:int(0.8 * N)]
    test_idx = idx[int(0.8 * N):]
    return X[:, train_idx], Y[:, train_idx], X[:, test_idx], Y[:, test_idx]

def train_and_plot(X, Y, hdim, label):
    X_train, Y_train, X_test, Y_test = split_once(X, Y)
    model = MLP(input_dim=1, hidden_dim=hdim, output_dim=1, learning_rate=0.01, epochs=2000)
    model.fit(X_train, Y_train, X_test, Y_test)

    plt.figure()
    plt.plot(model.loss_history, label="Erro Treino")
    plt.plot(model.test_loss_history, label="Erro Teste")
    plt.title(f"Curva de Aprendizado - MLP {label}")
    plt.xlabel("√âpocas")
    plt.ylabel("Erro Quadr√°tico M√©dio (MSE)")
    plt.legend()
    plt.grid(True)
    plt.show()

    print(f"MSE final (Treino) - {label}: {model.score(X_train, Y_train):.5f}")
    print(f"MSE final (Teste)  - {label}: {model.score(X_test, Y_test):.5f}")

# Teste: underfitting (1 neur√¥nio) vs overfitting (50 neur√¥nios)
train_and_plot(X_norm, Y_norm, hdim=2, label="(1 neur√¥nio - Underfitting)")
train_and_plot(X_norm, Y_norm, hdim=40, label="(50 neur√¥nios - Overfitting)")

"""###An√°lise das Curvas de Aprendizado ‚Äì Underfitting vs Overfitting

A MLP com 1 neur√¥nio oculto apresenta um comportamento cl√°ssico de underfitting. Por possuir uma topologia extremamente simples, o modelo n√£o consegue capturar a complexidade da rela√ß√£o entre entrada e sa√≠da, resultando em erros elevados tanto no treino quanto no teste. A curva de aprendizado revela uma r√°pida estabiliza√ß√£o do erro, mas com valores relativamente altos, evidenciando baixa capacidade de representa√ß√£o e generaliza√ß√£o.

Por outro lado, a MLP com 50 neur√¥nios na camada oculta demonstra potencial de overfitting, mas tamb√©m maior capacidade de ajuste. A curva de erro no treino √© visivelmente inferior √† do teste, com ambos os erros diminuindo ao longo das √©pocas. O desempenho mais acurado, especialmente no conjunto de teste, sugere que o modelo foi capaz de aprender padr√µes complexos com efici√™ncia. Ainda assim, a diferen√ßa entre as curvas deve ser monitorada para evitar sobreajuste em rodadas futuras.
"""

def monte_carlo_mlp(X, Y, R=100, hdim=5):
    mse_list = []
    N = X.shape[1]
    for _ in range(R):
        idx = np.random.permutation(N)
        train_idx = idx[:int(0.8 * N)]
        test_idx = idx[int(0.8 * N):]
        X_train, Y_train = X[:, train_idx], Y[:, train_idx]
        X_test, Y_test = X[:, test_idx], Y[:, test_idx]

        model = MLP(input_dim=1, hidden_dim=hdim, output_dim=1, learning_rate=0.01, epochs=500)
        model.fit(X_train, Y_train)
        mse = model.score(X_test, Y_test)
        mse_list.append(mse)
    return np.array(mse_list)

def monte_carlo_adaline(X, Y, R=100):
    mse_list = []
    N = X.shape[1]
    for _ in range(R):
        idx = np.random.permutation(N)
        train_idx = idx[:int(0.8 * N)]
        test_idx = idx[int(0.8 * N):]
        X_train, Y_train = X[:, train_idx], Y[:, train_idx]
        X_test, Y_test = X[:, test_idx], Y[:, test_idx]

        model = Adaline(input_dim=1, learning_rate=0.001, epochs=100)
        model.fit(X_train, Y_train)
        mse = model.score(X_test, Y_test)
        mse_list.append(mse)
    return np.array(mse_list)

def print_stats(arr, label):
    print(f"\n{label}")
    print(f"M√©dia: {np.mean(arr):.5f}")
    print(f"Desvio padr√£o: {np.std(arr):.5f}")
    print(f"M√°ximo: {np.max(arr):.5f}")
    print(f"M√≠nimo: {np.min(arr):.5f}")

"""A an√°lise Monte Carlo foi aplicada a duas topologias do MLP: uma subdimensionada (1 neur√¥nio ‚Äì underfitting) e outra superdimensionada (50 neur√¥nios ‚Äì overfitting). Os resultados mostram como o aumento da complexidade impacta o erro de generaliza√ß√£o e a variabilidade do desempenho entre diferentes divis√µes dos dados."""

mse_adaline = monte_carlo_adaline(X_norm, Y_norm)
mse_mlp_5 = monte_carlo_mlp(X_norm, Y_norm, hdim=5)

print_stats(mse_adaline, "ADALINE")
print_stats(mse_mlp_5, "MLP - 5")

idx = np.random.permutation(X_norm.shape[1])
train_idx = idx[:int(0.8 * X_norm.shape[1])]
test_idx = idx[int(0.8 * X_norm.shape[1]):]

X_train, Y_train = X_norm[:, train_idx], Y_norm[:, train_idx]
X_test, Y_test = X_norm[:, test_idx], Y_norm[:, test_idx]

mlp_under = MLP(1, 1, 1, 0.01, 1500)
mlp_under.fit(X_train, Y_train)
loss_train_under = mlp_under.loss_history
loss_test_under = [mlp_under.score(X_test, Y_test) for _ in loss_train_under]

mlp_over = MLP(1, 50, 1, 0.01, 1500)
mlp_over.fit(X_train, Y_train)
loss_train_over = mlp_over.loss_history
loss_test_over = [mlp_over.score(X_test, Y_test) for _ in loss_train_over]

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(loss_train_under, label='Treino')
plt.plot(loss_test_under, label='Teste')
plt.title("Underfitting - 1 neur√¥nio")
plt.xlabel("√âpocas")
plt.ylabel("MSE")
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(loss_train_over, label='Treino')
plt.plot(loss_test_over, label='Teste')
plt.title("Overfitting - 50 neur√¥nios")
plt.xlabel("√âpocas")
plt.ylabel("MSE")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""A diferen√ßa relativamente pequena entre os desempenhos do ADALINE e do MLP com 5 neur√¥nios se justifica pela baixa complexidade da topologia do MLP e pela poss√≠vel linearidade predominante na rela√ß√£o entre as vari√°veis. Embora o MLP ofere√ßa melhorias em termos de capacidade de modelagem, seu impacto √© limitado em contextos onde modelos lineares j√° conseguem se aproximar bem da fun√ß√£o alvo.

# üî∑ ETAPA 1 ‚Äì CLASSIFICA√á√ÉO BIN√ÅRIA

üé≤ Dados: (Spiral3d.csv)
"""

import numpy as np

try:
    data_spiral = np.loadtxt('/content/Spiral3d.csv', delimiter=',')
    data_spiral[:, 3] = np.where(data_spiral[:, 3] == 0, -1, data_spiral[:, 3])
    print("Arquivo Spiral3d.csv carregado com NumPy.")
    print(f"Shape dos dados: {data_spiral.shape}")
except FileNotFoundError:
    print("Arquivo Spiral3d.csv n√£o encontrado em /content/.")
except Exception as e:
    print(f"Ocorreu um erro ao carregar o arquivo: {e}")

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Carregar os dados
data = np.loadtxt('Spiral3d.csv', delimiter=',')
X = data[:, :3].T  # Transpor para formato (features, samples)
Y = data[:, 3].reshape(1, -1)  # R√≥tulos em formato (1, samples)

# Normaliza√ß√£o Min-Max para o intervalo [-1, 1]
X_min = X.min(axis=1, keepdims=True)
X_max = X.max(axis=1, keepdims=True)
X_norm = 2 * (X - X_min) / (X_max - X_min) - 1

# Adicionar bias como √∫ltima linha
X_bias = np.vstack((X_norm, -np.ones((1, X_norm.shape[1]))))

from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X_norm[0, Y[0]==1], X_norm[1, Y[0]==1], X_norm[2, Y[0]==1], c='r', cmap='viridis', edgecolor='k', linewidth=0.8, alpha=0.9, label='Classe +1')
ax.scatter(X_norm[0, Y[0]==-1], X_norm[1, Y[0]==-1], X_norm[2, Y[0]==-1], c='b', cmap='viridis', edgecolor='k', linewidth=0.8, alpha=0.9, label='Classe -1')
ax.set_title('Distribui√ß√£o das Classes no Espa√ßo 3D')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('X3')
ax.legend()
plt.show()

"""Coment√°rio: O gr√°fico acima mostra a distribui√ß√£o tridimensional das classes ap√≥s a normaliza√ß√£o. Observa-se que as classes n√£o s√£o linearmente separ√°veis, indicando a necessidade de modelos n√£o lineares para uma classifica√ß√£o eficaz.

# üìò Perceptron Simples:

- √â um modelo linear, com fun√ß√£o de ativa√ß√£o degrau:

$$y = \text{sign}(w \cdot x + b)$$

- Aprende ajustando os pesos com:

$$w \leftarrow w + \eta \cdot (y_{\text{real}} - y_{\text{previsto}}) \cdot x$$

- Converge somente se os dados forem linearmente separ√°veis (n√£o √© o caso aqui, por isso √© importante estudar seu desempenho).
"""

class Perceptron:
    def __init__(self, input_dim=4, learning_rate=0.01, epochs=500):
        self.weights = np.random.randn(1, input_dim) * 0.01
        self.bias = 0.0
        self.lr = learning_rate
        self.epochs = epochs
        self.loss_history = []

    def sign(self, x):
        return np.where(x >= 0, 1, -1)

    def predict(self, X):
        return self.sign(np.dot(self.weights, X) + self.bias)

    def fit(self, X, Y):
        N = X.shape[1]
        self.loss_history = []

        for _ in range(self.epochs):
            Y_pred = self.predict(X)
            errors = Y - Y_pred
            updates = np.dot(errors, X.T)
            self.weights += self.lr * updates
            self.bias += self.lr * np.sum(errors)
            self.loss_history.append(np.mean(errors != 0))



"""O Perceptron Simples foi implementado com fun√ß√£o de ativa√ß√£o do tipo degrau (sinal), classificando as amostras com base no sinal da soma ponderada dos pesos. A cada √©poca, os pesos s√£o atualizados apenas quando ocorrem erros de classifica√ß√£o, conforme a Regra do Perceptron. Por se tratar de um modelo linear, sua capacidade de generaliza√ß√£o √© limitada em problemas com fronteiras n√£o lineares, como √© o caso do Spiral3D."""

# Divis√£o treino/teste (80/20)
np.random.seed(42)
idx = np.random.permutation(X_norm.shape[1])
split = int(0.8 * X_norm.shape[1])
X_train = X_norm[:, idx[:split]]
Y_train = Y[:, idx[:split]]
X_test = X_norm[:, idx[split:]]
Y_test = Y[:, idx[split:]]

# Instanciar e treinar o modelo
perceptron = Perceptron(input_dim=3, learning_rate=0.01, epochs=200)
perceptron.fit(X_train, Y_train)

# Curva de aprendizado
plt.plot(perceptron.loss_history)
plt.title("Curva de Aprendizado - Perceptron Simples")
plt.xlabel("√âpocas")
plt.ylabel("Taxa de Erro")
plt.grid(True)
plt.show()

# Predi√ß√µes
Y_pred = perceptron.predict(X_test)

# M√©tricas
def confusion_matrix_manual(Y_true, Y_pred):
    tp = np.sum((Y_true == 1) & (Y_pred == 1))
    tn = np.sum((Y_true == -1) & (Y_pred == -1))
    fp = np.sum((Y_true == -1) & (Y_pred == 1))
    fn = np.sum((Y_true == 1) & (Y_pred == -1))
    return tp, tn, fp, fn

tp, tn, fp, fn = confusion_matrix_manual(Y_test.flatten(), Y_pred.flatten())

accuracy = (tp + tn) / (tp + tn + fp + fn)
sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
specificity = tn / (tn + fp) if (tn + fp) > 0 else 0

print(f"Acur√°cia: {accuracy:.4f}")
print(f"Sensibilidade (Classe +1): {sensitivity:.4f}")
print(f"Especificidade (Classe -1): {specificity:.4f}")
print(f"Matriz de Confus√£o:")
print(f"[[{tp} (TP), {fn} (FN)]\n [{fp} (FP), {tn} (TN)]]")

"""O gr√°fico mostra que o Perceptron teve dificuldade em classificar corretamente a classe -1, apresentando baixa especificidade. J√° a classe +1 foi bem reconhecida, com alta sensibilidade. Isso confirma a limita√ß√£o do modelo em problemas com separa√ß√£o n√£o linear, como no conjunto Spiral3D.


"""

class MLPClassifier:
    def __init__(self, input_dim, hidden_dims, learning_rate=0.01, epochs=1000):
        self.layers = [input_dim] + hidden_dims + [1]  # incluindo entrada e sa√≠da
        self.lr = learning_rate
        self.epochs = epochs
        self.loss_history = []

        # Inicializar pesos e bias para cada camada
        self.W = []
        self.b = []
        for i in range(len(self.layers) - 1):
            limit = np.sqrt(6 / (self.layers[i] + self.layers[i+1]))
            self.W.append(np.random.uniform(-limit, limit, (self.layers[i+1], self.layers[i])))
            self.b.append(np.zeros((self.layers[i+1], 1)))

    def tanh(self, z):
        return np.tanh(z)

    def tanh_derivative(self, z):
        return 1 - np.tanh(z) ** 2

    def forward(self, X):
        A = X
        activations = [X]
        Zs = []

        for i in range(len(self.W) - 1):  # camadas ocultas
            Z = np.dot(self.W[i], A) + self.b[i]
            A = self.tanh(Z)
            Zs.append(Z)
            activations.append(A)

        # Camada de sa√≠da (linear)
        Z = np.dot(self.W[-1], A) + self.b[-1]
        activations.append(Z)
        Zs.append(Z)

        return Zs, activations

    def predict_raw(self, X):
        _, activations = self.forward(X)
        return activations[-1]

    def predict(self, X):
        return np.where(self.predict_raw(X) >= 0, 1, -1)

    def fit(self, X, Y):
        N = X.shape[1]

        for _ in range(self.epochs):
            # FORWARD
            Zs, activations = self.forward(X)
            Y_pred = activations[-1]

            # Loss
            error = Y_pred - Y
            self.loss_history.append(np.mean(error ** 2))

            # BACKWARD
            dA = error
            dWs = [None] * len(self.W)
            dbs = [None] * len(self.b)

            for i in reversed(range(len(self.W))):
                Z = Zs[i]
                A_prev = activations[i]

                if i == len(self.W) - 1:
                    dZ = dA  # sa√≠da linear
                else:
                    dZ = dA * self.tanh_derivative(Z)

                dW = (1 / N) * np.dot(dZ, A_prev.T)
                db = (1 / N) * np.sum(dZ, axis=1, keepdims=True)

                dA = np.dot(self.W[i].T, dZ)

                dWs[i] = dW
                dbs[i] = db

            # Atualiza√ß√£o dos pesos
            for i in range(len(self.W)):
                self.W[i] -= self.lr * dWs[i]
                self.b[i] -= self.lr * dbs[i]

"""#üîÑ Diferen√ßas principais para classifica√ß√£o bin√°ria:

- A fun√ß√£o de ativa√ß√£o da sa√≠da ser√° a tangente hiperb√≥lica (tanh), pois estamos trabalhando com r√≥tulos {-1, +1}.

- A decis√£o final ser√° baseada no sinal da sa√≠da:

$$\hat{y} = \text{sign}(\text{MLP}(x))$$
"""

# Instanciar e treinar o modelo
mlp = MLPClassifier(input_dim=3, hidden_dims=[10], learning_rate=0.01, epochs=2000)
mlp.fit(X_train, Y_train)

# Curva de aprendizado
plt.plot(mlp.loss_history)
plt.title("Curva de Aprendizado - MLP (10 neur√¥nios) ")
plt.xlabel("√âpocas")
plt.ylabel("Erro Quadr√°tico M√©dio (MSE)")
plt.grid(True)
plt.show()

# Predi√ß√£o
Y_pred_mlp = mlp.predict(X_test)

# M√©tricas
tp, tn, fp, fn = confusion_matrix_manual(Y_test.flatten(), Y_pred_mlp.flatten())

accuracy = (tp + tn) / (tp + tn + fp + fn)
sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
specificity = tn / (tn + fp) if (tn + fp) > 0 else 0

print(f"Acur√°cia: {accuracy:.4f}")
print(f"Sensibilidade (Classe +1): {sensitivity:.4f}")
print(f"Especificidade (Classe -1): {specificity:.4f}")
print(f"Matriz de Confus√£o:")
print(f"[[{tp} (TP), {fn} (FN)]\n [{fp} (FP), {tn} (TN)]]")

"""A curva de aprendizado do MLP indica uma redu√ß√£o est√°vel do erro ao longo das √©pocas. As m√©tricas de avalia√ß√£o mostram melhora clara em rela√ß√£o ao Perceptron Simples, com aumento tanto na sensibilidade quanto na especificidade. Isso confirma a superioridade dos modelos n√£o lineares para problemas com fronteiras de decis√£o complexas, como o Spiral3D."""

def monte_carlo_classification(model_type="perceptron", hidden_dim=10, R=100):
    acc_list, sens_list, spec_list = [], [], []
    N = X_norm.shape[1]

    for _ in range(R):
        idx = np.random.permutation(N)
        split = int(0.8 * N)
        X_train = X_norm[:, idx[:split]]
        Y_train = Y[:, idx[:split]]
        X_test = X_norm[:, idx[split:]]
        Y_test = Y[:, idx[split:]]

        if model_type == "perceptron":
            model = Perceptron(input_dim=3, learning_rate=0.01, epochs=500)
        elif model_type == "mlp":
            model = MLPClassifier(input_dim=3, hidden_dims=[5,5,5], learning_rate=0.01, epochs=500)

        model.fit(X_train, Y_train)
        Y_pred = model.predict(X_test)

        tp, tn, fp, fn = confusion_matrix_manual(Y_test.flatten(), Y_pred.flatten())
        acc = (tp + tn) / (tp + tn + fp + fn)
        sens = tp / (tp + fn) if (tp + fn) > 0 else 0
        spec = tn / (tn + fp) if (tn + fp) > 0 else 0

        acc_list.append(acc)
        sens_list.append(sens)
        spec_list.append(spec)

    return np.array(acc_list), np.array(sens_list), np.array(spec_list)

def print_metrics_fixed(name, acc, sens, spec):
    print(f"\n=== {name} ===")
    print("Acur√°cia:")
    print("  M√©dia: {:.4f} | DP: {:.4f} | Max: {:.4f} | Min: {:.4f}".format(np.mean(acc), np.std(acc), np.max(acc), np.min(acc)))
    print("Sensibilidade:")
    print("  M√©dia: {:.4f} | DP: {:.4f} | Max: {:.4f} | Min: {:.4f}".format(np.mean(sens), np.std(sens), np.max(sens), np.min(sens)))
    print("Especificidade:")
    print("  M√©dia: {:.4f} | DP: {:.4f} | Max: {:.4f} | Min: {:.4f}".format(np.mean(spec), np.std(spec), np.max(spec), np.min(spec)))

acc_p, sens_p, spec_p = monte_carlo_classification("perceptron", R=100)
acc_m, sens_m, spec_m = monte_carlo_classification("mlp", hidden_dim=[10], R=100)

print_metrics_fixed("Perceptron Simples (100 rodadas)", acc_p, sens_p, spec_p)
print_metrics_fixed("MLP (10 neur√¥nios, 100 rodadas)", acc_m, sens_m, spec_m)



# Parte 6: Gerar boxplots com matplotlib para acur√°cia, sensibilidade e especificidade

def plot_boxplots(metrics, labels, title):
    fig, ax = plt.subplots()
    ax.boxplot(metrics, vert=True, patch_artist=True, labels=labels)
    ax.set_title(title)
    ax.set_ylabel("Valor")
    plt.grid(True)
    plt.show()

# M√©tricas para boxplots
plot_boxplots([acc_p, acc_m], ["Perceptron", "MLP"], "Boxplot de Acur√°cia")
plot_boxplots([sens_p, sens_m], ["Perceptron", "MLP"], "Boxplot de Sensibilidade")
plot_boxplots([spec_p, spec_m], ["Perceptron", "MLP"], "Boxplot de Especificidade")

"""#üìä Interpreta√ß√£o dos Boxplots

##Acur√°cia:

O MLP apresentou m√©dia e mediana superiores em rela√ß√£o ao Perceptron Simples, com desempenho mais elevado em diversas rodadas. No entanto, sua dispers√£o tamb√©m foi maior, indicando uma maior sensibilidade √† varia√ß√£o nas divis√µes dos dados. O Perceptron, embora limitado, mostrou distribui√ß√£o mais concentrada e menor variabilidade.

##Sensibilidade:

O MLP obteve m√©dia consideravelmente mais alta e est√°vel, refletindo melhor capacidade de reconhecer corretamente os exemplos da classe positiva. O Perceptron, por outro lado, teve sensibilidade extremamente inst√°vel, com valores variando de 0.11 a 1.00, o que compromete sua confiabilidade.

##Especificidade:

O Perceptron teve desempenho mais concentrado, mas essa concentra√ß√£o se deu entre valores extremos. J√° o MLP apresentou m√©dia pr√≥xima a 0.62, com melhor equil√≠brio e consist√™ncia, embora com alguns casos de baixo desempenho. Isso refor√ßa a maior robustez do MLP na identifica√ß√£o da classe negativa.
"""

# Parte 7: Matriz de confus√£o para melhor e pior caso da MLP

# Fun√ß√£o Monte Carlo que retorna acur√°cias e matrizes
def monte_carlo_with_loss(X, Y, hidden_dim=10, R=100):
    acc_list = []
    conf_matrices = []
    loss_histories = []
    N = X.shape[1]

    for _ in range(R):
        idx = np.random.permutation(N)
        split = int(0.8 * N)
        X_train, Y_train = X[:, idx[:split]], Y[:, idx[:split]]
        X_test, Y_test = X[:, idx[split:]], Y[:, idx[split:]]

        # Alterar input_dim de 3 para 4 para incluir o termo de bias adicionado em X_bias
        model = MLPClassifier(input_dim=4, hidden_dims=[10], learning_rate=0.01, epochs=500)
        model.fit(X_train, Y_train)
        Y_pred = model.predict(X_test)

        tp, tn, fp, fn = confusion_matrix_manual(Y_test.flatten(), Y_pred.flatten())
        acc = (tp + tn) / (tp + tn + fp + fn)
        acc_list.append(acc)
        conf_matrices.append(np.array([[tp, fn], [fp, tn]]))
        loss_histories.append(model.loss_history)

    return np.array(acc_list), conf_matrices, loss_histories

# Executar e encontrar melhor/pior matriz
acc_all, conf_all, loss_all = monte_carlo_with_loss(X_bias, Y, hidden_dim=4, R=100)

best_idx = np.argmax(acc_all)
worst_idx = np.argmin(acc_all)

conf_best = conf_all[best_idx]
conf_worst = conf_all[worst_idx]
loss_best = loss_all[best_idx]
loss_worst = loss_all[worst_idx]

# As fun√ß√µes plot_confusion_matrix_seaborn e confusion_matrix_manual

def plot_confusion_matrix_seaborn(matrix, title, accuracy):
    labels = ["+1", "-1"]
    plt.figure(figsize=(6, 5))
    sns.heatmap(matrix, annot=True, fmt='d', cmap="Blues" if "Melhor" in title else "Reds",
                xticklabels=labels, yticklabels=labels[::-1], cbar=True)
    plt.xlabel("Predito")
    plt.ylabel("Real")
    plt.title(f"{title}\nAcur√°cia = {accuracy:.2f}%")
    plt.tight_layout()
    plt.show()

plot_confusion_matrix_seaborn(conf_best, "Melhor Acur√°cia - MLP", acc_all[best_idx] * 100)
plot_confusion_matrix_seaborn(conf_worst, "Pior Acur√°cia - MLP", acc_all[worst_idx] * 100)

# Curvas de aprendizado reais com porcentagem de acur√°cia no r√≥tulo
plt.figure(figsize=(8, 5))
plt.plot(loss_best, label=f'Melhor Acur√°cia ({acc_all[best_idx]*100:.2f}%)')
plt.plot(loss_worst, label=f'Pior Acur√°cia ({acc_all[worst_idx]*100:.2f}%)')
plt.title("Curvas de Aprendizado Reais - MLP (10 Neur√¥nios)") # Note: T√≠tulo ainda indica 10 neur√¥nios, mas a chamada usa 20. Pode querer ajustar.
plt.xlabel("√âpocas")
plt.ylabel("Erro Quadr√°tico M√©dio (MSE)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""#üü¶ Melhor Acur√°cia - MLP
A matriz de confus√£o mostra que a rede obteve √≥timo desempenho em ambas as classes, com alta taxa de acertos e baixa confus√£o. Isso demonstra que, quando a divis√£o dos dados √© favor√°vel, o MLP √© capaz de capturar padr√µes complexos de forma eficiente, mesmo em um problema n√£o linear como o Spiral3D.

#üü• Pior Acur√°cia - MLP
Neste cen√°rio, observa-se uma alta quantidade de falsos positivos e negativos, indicando dificuldade da rede em generalizar. Isso pode ser consequ√™ncia de uma divis√£o desfavor√°vel dos dados, overfitting ou limita√ß√£o moment√¢nea da topologia. A curva de aprendizado associada tamb√©m mostra converg√™ncia lenta e erro elevado, refor√ßando a dificuldade de aprendizado nesta rodada.

# üî∑ ETAPA 2 ‚Äì CLASSIFICA√á√ÉO MULTICLASSE

üé≤ Dados: (coluna_vertebral.csv)

## Objetivo:

#### Classificar pacientes em 3 categorias cl√≠nicas:

- Normal

- H√©rnia de Disco

- Espondilolistese
"""

import numpy as np

try:
    data_coluna = np.genfromtxt('/content/coluna_vertebral.csv', delimiter=',', skip_header=1)
    print("Arquivo coluna_vertebral.csv carregado com NumPy (ignorando o cabe√ßalho).")
    print(f"Shape dos dados: {data_coluna.shape}")
except FileNotFoundError:
    print("Arquivo coluna_vertebral.csv n√£o encontrado em /content/.")
except Exception as e:
    print(f"Ocorreu um erro ao carregar o arquivo: {e}")

# Carregar o CSV (assumindo cabe√ßalho e r√≥tulo na √∫ltima coluna)
data = np.genfromtxt("coluna_vertebral.csv", delimiter=',', dtype=str)

# Separar entradas e r√≥tulos
X_raw = data[1:, :-1].astype(float).T  # X ‚àà ‚Ñù‚Å∂√óN (remo√ß√£o do cabe√ßalho)
Y_text = data[1:, -1]  # r√≥tulos string

# Mapeamento correto dos r√≥tulos one-hot {-1, +1}
label_map = {
    "NO": np.array([[+1], [-1], [-1]]),  # Normal
    "DH": np.array([[-1], [+1], [-1]]),  # H√©rnia de Disco
    "SL": np.array([[-1], [-1], [+1]])   # Espondilolistese
}

# Codifica√ß√£o dos r√≥tulos
Y_encoded = np.hstack([label_map[label] for label in Y_text])  # Y ‚àà ‚Ñù¬≥√óN

# Adicionar vi√©s artificial (bias = 1)
bias = np.ones((1, X_raw.shape[1]))
X_bias = np.vstack([X_raw, bias])  # X ‚àà ‚Ñù‚Å∑√óN

# Calcular X_min and X_max com base em X_bias, e n√£o X_raw
X_min = X_bias.min(axis=1, keepdims=True)
X_max = X_bias.max(axis=1, keepdims=True)


# Normaliza√ß√£o Min-Max [-1, +1] com prote√ß√£o contra divis√£o por zero
denom = X_max - X_min
denom = np.where(denom == 0, 1, denom)  # evita divis√£o por zero
X_norm = 2 * (X_bias - X_min) / denom - 1

# Checagem de formas
print("X shape:", X_norm.shape)
print("Y shape:", Y_encoded.shape)

"""###üìò Modelo ADALINE ‚Äì Classifica√ß√£o Multiclasse
üéØ Fun√ß√£o de Custo:

$$\text{Erro} = \frac{1}{N} \sum_{i=1}^{N} \|Y_i - (WX_i + b)\|^2$$

"""

# Modelo ADALINE para multiclasse
class AdalineMulticlass:
    def __init__(self, input_dim, output_dim, learning_rate=0.01, epochs=200):
        self.W = np.random.randn(output_dim, input_dim) * 0.01
        self.b = np.zeros((output_dim, 1))
        self.lr = learning_rate
        self.epochs = epochs
        self.loss_history = []

    def predict(self, X):
        scores = np.dot(self.W, X) + self.b
        max_indices = np.argmax(scores, axis=0)
        Y_pred = -np.ones_like(scores)
        for i, idx in enumerate(max_indices):
            Y_pred[idx, i] = 1
        return Y_pred

    def fit(self, X, Y):
        N = X.shape[1]
        for _ in range(self.epochs):
            Y_pred = np.dot(self.W, X) + self.b
            error = Y - Y_pred
            error = np.clip(error, -1e6, 1e6)
            mse = np.mean(error ** 2)
            if np.isnan(mse) or np.isinf(mse):
                print("Erro num√©rico detectado. Interrompendo treino.")
                break
            self.loss_history.append(mse)

            dW = -(2 / N) * np.dot(error, X.T)
            db = -(2 / N) * np.sum(error, axis=1, keepdims=True)

            self.W -= self.lr * dW
            self.b -= self.lr * db

# Divis√£o treino/teste (80/20)
np.random.seed(42)
X_norm_T = X_norm.T  # X ‚àà ‚Ñù‚Å∑√óN ‚Üí N√ó7 (para embaralhar amostras)
Y_encoded_T = Y_encoded.T  # Y ‚àà ‚Ñù¬≥√óN ‚Üí N√ó3

num_samples = min(X_norm.shape[1], Y_encoded.shape[1])
idx = np.random.permutation(num_samples)
split = int(0.8 * num_samples)

# Divis√£o e transposi√ß√£o de volta para treino
X_train = X_norm[:, idx[:split]]
Y_train = Y_encoded[:, idx[:split]]
X_test = X_norm[:, idx[split:]]
Y_test = Y_encoded[:, idx[split:]]

# Treinamento do ADALINE
adaline = AdalineMulticlass(input_dim=7, output_dim=3, learning_rate=0.01, epochs=200)
adaline.fit(X_train, Y_train)

# Curva de aprendizado
plt.plot(adaline.loss_history)
plt.title("Curva de Aprendizado ‚Äì ADALINE Multiclasse")
plt.xlabel("√âpocas")
plt.ylabel("Erro Quadr√°tico M√©dio (MSE)")
plt.grid(True)
plt.tight_layout()
plt.show()

# Acur√°cia
Y_pred = adaline.predict(X_test)
accuracy = np.mean(np.all(Y_pred == Y_test, axis=0))
print(f"Acur√°cia do ADALINE Multiclasse: {accuracy:.4f}")

"""###üìä Resultado ‚Äì ADALINE Multiclasse
Acur√°cia: 66%

Curva de Erro: Redu√ß√£o consistente do MSE ao longo das 200 √©pocas

O ADALINE Multiclasse apresentou aprendizado est√°vel, com redu√ß√£o consistente do MSE ao longo das √©pocas. Embora seja um modelo linear, obteve uma acur√°cia final de 66%, bem superior √† taxa esperada por classifica√ß√£o aleat√≥ria (‚âà33%), o que indica que o modelo foi capaz de capturar parte das rela√ß√µes entre os atributos. No entanto, por n√£o incorporar mecanismos n√£o lineares, tende a apresentar limita√ß√µes frente a padr√µes mais complexos, como os da base cl√≠nica da coluna vertebral ‚Äî refor√ßando a necessidade de modelos como o MLP para maior capacidade de generaliza√ß√£o.
"""

# Modelo MLP para multiclasse
class MLPMulticlass:
    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01, epochs=200):
        self.W1 = np.random.randn(hidden_dim, input_dim) * 0.01
        self.b1 = np.zeros((hidden_dim, 1))
        self.W2 = np.random.randn(output_dim, hidden_dim) * 0.01
        self.b2 = np.zeros((output_dim, 1))
        self.lr = learning_rate
        self.epochs = epochs
        self.loss_history = []

    def tanh(self, z):
        return np.tanh(z)

    def tanh_derivative(self, z):
        return 1 - np.tanh(z) ** 2

    def predict(self, X):
        Z1 = np.dot(self.W1, X) + self.b1
        A1 = self.tanh(Z1)
        Z2 = np.dot(self.W2, A1) + self.b2
        scores = Z2
        max_indices = np.argmax(scores, axis=0)
        Y_pred = -np.ones_like(scores)
        for i, idx in enumerate(max_indices):
            Y_pred[idx, i] = 1
        return Y_pred

    def fit(self, X, Y):
        N = X.shape[1]
        for _ in range(self.epochs):
            Z1 = np.dot(self.W1, X) + self.b1
            A1 = self.tanh(Z1)
            Z2 = np.dot(self.W2, A1) + self.b2
            Y_pred = Z2

            error = Y_pred - Y
            mse = np.mean(error ** 2)
            self.loss_history.append(mse)

            dZ2 = error
            dW2 = (1 / N) * np.dot(dZ2, A1.T)
            db2 = (1 / N) * np.sum(dZ2, axis=1, keepdims=True)

            dA1 = np.dot(self.W2.T, dZ2)
            dZ1 = dA1 * self.tanh_derivative(Z1)
            dW1 = (1 / N) * np.dot(dZ1, X.T)
            db1 = (1 / N) * np.sum(dZ1, axis=1, keepdims=True)

            self.W2 -= self.lr * dW2
            self.b2 -= self.lr * db2
            self.W1 -= self.lr * dW1
            self.b1 -= self.lr * db1

# Treinamento da MLP
mlp = MLPMulticlass(input_dim=7, hidden_dim=10, output_dim=3, learning_rate=0.01, epochs=500)
mlp.fit(X_train, Y_train)

# Curva de aprendizado
plt.plot(mlp.loss_history)
plt.title("Curva de Aprendizado - MLP Multiclasse")
plt.xlabel("√âpocas")
plt.ylabel("Erro Quadr√°tico M√©dio (MSE)")
plt.grid(True)
plt.tight_layout()
plt.show()

# Acur√°cia
Y_pred_mlp = mlp.predict(X_test)
accuracy_mlp = np.mean(np.all(Y_pred_mlp == Y_test, axis=0))
print(f"Acur√°cia da MLP Multiclasse: {accuracy_mlp:.4f}")

"""Embora o MLP Multiclasse tenha apresentado uma curva de erro com tend√™ncia clara de converg√™ncia, sua acur√°cia final (45,16%) ficou significativamente abaixo do desempenho obtido com o ADALINE Multiclasse. Isso sugere que o modelo pode n√£o ter alcan√ßado uma boa generaliza√ß√£o, possivelmente devido √† configura√ß√£o de hiperpar√¢metros, topologia ou √† complexidade do problema frente √† arquitetura utilizada."""

# Fun√ß√£o de m√©tricas multiclasse
def calcular_metricas_multiclasse(Y_true, Y_pred):
    TP = np.sum(np.logical_and(Y_true == 1, Y_pred == 1), axis=1)
    TN = np.sum(np.logical_and(Y_true == -1, Y_pred == -1), axis=1)
    FP = np.sum(np.logical_and(Y_true == -1, Y_pred == 1), axis=1)
    FN = np.sum(np.logical_and(Y_true == 1, Y_pred == -1), axis=1)

    sens = TP / (TP + FN + 1e-10)
    spec = TN / (TN + FP + 1e-10)

    sensibilidade = np.mean(sens)
    especificidade = np.mean(spec)
    return sensibilidade, especificidade

def monte_carlo_multiclasse(model_class, R=100, hidden_dim=10):
    acc_list, sens_list, spec_list = [], [], []
    N = X_norm.shape[1]  # Total number of samples
    num_samples = Y_encoded.shape[1] # Number of samples for Y_encoded

    for _ in range(R):
        # Generate random indices within the range of Y_encoded
        idx = np.random.permutation(num_samples)
        split = int(0.8 * num_samples)  # Split based on Y_encoded size
        X_train_idx = np.random.choice(N, split, replace=False)  # Randomly select X_train indices
        X_test_idx = np.setdiff1d(np.arange(N), X_train_idx)  # Select remaining indices for X_test

        X_train = X_norm[:, X_train_idx]
        Y_train = Y_encoded[:, idx[:split]]
        X_test = X_norm[:, X_test_idx]
        Y_test = Y_encoded[:, idx[split:]]

        if model_class.__name__ == "MLPMulticlass":
            model = model_class(input_dim=7, hidden_dim=hidden_dim, output_dim=3, learning_rate=0.01, epochs=500)
        else:
            model = model_class(input_dim=7, output_dim=3, learning_rate=0.01, epochs=500)

        model.fit(X_train, Y_train)
        Y_pred = model.predict(X_test)

        acc = np.mean(np.all(Y_pred == Y_test, axis=0))
        sens, spec = calcular_metricas_multiclasse(Y_test, Y_pred)

        acc_list.append(acc)
        sens_list.append(sens)
        spec_list.append(spec)

    return np.array(acc_list), np.array(sens_list), np.array(spec_list)

# Executar Monte Carlo para ADALINE e MLP com R=100
acc_adaline, sens_adaline, spec_adaline = monte_carlo_multiclasse(AdalineMulticlass, R=100)
acc_mlp, sens_mlp, spec_mlp = monte_carlo_multiclasse(MLPMulticlass, R=100, hidden_dim=10)

# Fun√ß√£o para imprimir tabela no console
def print_tabela(nome, valores):
    print(f"\nüìä {nome}")
    print(f"M√©dia: {np.mean(valores):.4f}")
    print(f"Desvio padr√£o: {np.std(valores):.4f}")
    print(f"M√°ximo: {np.max(valores):.4f}")
    print(f"M√≠nimo: {np.min(valores):.4f}")

# Tabelas de resultados
print_tabela("Acur√°cia - ADALINE", acc_adaline)
print_tabela("Sensibilidade - ADALINE", sens_adaline)
print_tabela("Especificidade - ADALINE", spec_adaline)

print_tabela("Acur√°cia - MLP", acc_mlp)
print_tabela("Sensibilidade - MLP", sens_mlp)
print_tabela("Especificidade - MLP", spec_mlp)

# Boxplot comparativo de Acur√°cia
plt.figure(figsize=(6, 4))
plt.boxplot([acc_adaline, acc_mlp], labels=["ADALINE", "MLP"], patch_artist=True)
plt.title("Boxplot de Acur√°cia ‚Äì ADALINE vs MLP")
plt.ylabel("Acur√°cia")
plt.grid(True)
plt.tight_layout()
plt.show()

"""# An√°lise:

Mesmo com 100 rodadas, o ADALINE apresentou desempenho mais equilibrado, com varia√ß√£o natural nas m√©tricas e boa generaliza√ß√£o para o problema multiclasse. J√° o MLP obteve acur√°cia m√©dia ligeiramente superior, mas com sensibilidade e especificidade constantes, indicando que o modelo pode estar priorizando uma √∫nica classe nas predi√ß√µes. Isso mostra que, mesmo sendo mais simples, o ADALINE foi mais est√°vel e eficaz nesse cen√°rio.




"""

# Parte 5 ‚Äì Identificar melhor e pior caso (MLP) e gerar matriz de confus√£o + curva

# Encontrar √≠ndices de melhor e pior acur√°cia da MLP
best_idx = np.argmax(acc_mlp)
worst_idx = np.argmin(acc_mlp)


# Fun√ß√£o para rodar novamente um √≠ndice espec√≠fico
def avaliar_mlp_em_idx(idx, hidden_dim=10):
    N = X_norm.shape[1]
    idx_perm = np.random.permutation(N)
    split = int(0.8 * N)
    X_train = X_norm[:, idx_perm[:split]]
    Y_train = Y_encoded[:, idx_perm[:split]]
    X_test = X_norm[:, idx_perm[split:]]
    Y_test = Y_encoded[:, idx_perm[split:]]

    model = MLPMulticlass(input_dim=7, hidden_dim=hidden_dim, output_dim=3, learning_rate=0.01, epochs=200)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)

    # Calculate accuracy for this specific run
    acc = np.mean(np.all(Y_pred == Y_test, axis=0))

    return Y_test, Y_pred, model.loss_history, acc

# Executar melhor e pior
Y_true_best, Y_pred_best, loss_best, acc_best_m = avaliar_mlp_em_idx(best_idx)
Y_true_worst, Y_pred_worst, loss_worst, acc_worst_m = avaliar_mlp_em_idx(worst_idx)

# Fun√ß√£o de matriz de confus√£o manual (linhas = reais, colunas = preditas)
def matriz_confusao(Y_true, Y_pred):
    true_labels = np.argmax(Y_true, axis=0)
    pred_labels = np.argmax(Y_pred, axis=0)
    num_classes = Y_true.shape[0]
    mat = np.zeros((num_classes, num_classes), dtype=int)
    for t, p in zip(true_labels, pred_labels):
        mat[t, p] += 1
    return mat

conf_best = matriz_confusao(Y_true_best, Y_pred_best)
conf_worst = matriz_confusao(Y_true_worst, Y_pred_worst)

import seaborn as sns
import matplotlib.pyplot as plt

# Plotar matrizes de confus√£o
def plot_confusion(mat, title):
    plt.figure(figsize=(5, 4))
    sns.heatmap(mat, annot=True, fmt="d", cmap="Blues", xticklabels=["Normal", "H√©rnia", "Espondilolistese"],
                yticklabels=["Normal", "H√©rnia", "Espondilolistese"])
    plt.xlabel("Classe Predita")
    plt.ylabel("Classe Real")
    plt.title(title)
    plt.tight_layout()
    plt.show()

# Use the calculated accuracies in the plot titles
plot_confusion(conf_best, f"Matriz de Confus√£o ‚Äì Melhor Acur√°cia MLP ({acc_best_m*100:.2f}%)")
plot_confusion(conf_worst, f"Matriz de Confus√£o ‚Äì Pior Acur√°cia MLP ({acc_worst_m*100:.2f}%)")

# Plotar curvas de aprendizado
plt.plot(loss_best, label=f"Melhor Acur√°cia ({acc_best_m*100:.2f}%)")
plt.plot(loss_worst, label=f"Pior Acur√°cia ({acc_worst_m*100:.2f}%)")
plt.title("Curvas de Aprendizado ‚Äì Melhor vs Pior Acur√°cia (MLP)")
plt.xlabel("√âpocas")
plt.ylabel("MSE")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

def avaliar_modelo_em_idx(model_class, idx, hidden_dim=10):
    N = X_norm.shape[1]
    idx_perm = np.random.permutation(N)
    split = int(0.8 * N)
    X_train = X_norm[:, idx_perm[:split]]
    Y_train = Y_encoded[:, idx_perm[:split]]
    X_test = X_norm[:, idx_perm[split:]]
    Y_test = Y_encoded[:, idx_perm[split:]]

    if model_class.__name__ == "MLPMulticlass":
        model = model_class(input_dim=7, hidden_dim=hidden_dim, output_dim=3, learning_rate=0.01, epochs=200)
    else:
        model = model_class(input_dim=7, output_dim=3, learning_rate=0.01, epochs=200)

    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)

    return Y_test, Y_pred, model.loss_history

best_idx_adaline = np.argmax(acc_adaline)
worst_idx_adaline = np.argmin(acc_adaline)

# Avalia√ß√£o ADALINE
Y_true_best_a, Y_pred_best_a, loss_best_a = avaliar_modelo_em_idx(AdalineMulticlass, best_idx_adaline)
Y_true_worst_a, Y_pred_worst_a, loss_worst_a = avaliar_modelo_em_idx(AdalineMulticlass, worst_idx_adaline)

# Matrizes
conf_best_a = matriz_confusao(Y_true_best_a, Y_pred_best_a)
conf_worst_a = matriz_confusao(Y_true_worst_a, Y_pred_worst_a)

# C√°lculo das acur√°cias para as legendas
acc_best_a = np.mean(np.all(Y_pred_best_a == Y_true_best_a, axis=0)) * 100
acc_worst_a = np.mean(np.all(Y_pred_worst_a == Y_true_worst_a, axis=0)) * 100

# Redefine plot_confusion to accept a cmap argument
def plot_confusion(mat, title, cmap="Blues"):
    plt.figure(figsize=(5, 4))
    sns.heatmap(mat, annot=True, fmt="d", cmap=cmap, xticklabels=["Normal", "H√©rnia", "Espondilolistese"],
                yticklabels=["Normal", "H√©rnia", "Espondilolistese"])
    plt.xlabel("Classe Predita")
    plt.ylabel("Classe Real")
    plt.title(title)
    plt.tight_layout()
    plt.show()

# Use the redefined plot_confusion with cmap argument
plot_confusion(conf_best_a, f"Matriz de Confus√£o ‚Äì Melhor Acur√°cia ADALINE ({acc_best_a:.2f}%)", "Greens")
plot_confusion(conf_worst_a, f"Matriz de Confus√£o ‚Äì Pior Acur√°cia ADALINE ({acc_worst_a:.2f}%)", "Reds")

# Curvas de aprendizado com % na legenda
plt.plot(loss_best_a, label=f"Melhor Acur√°cia ({acc_best_a:.2f}%)")
plt.plot(loss_worst_a, label=f"Pior Acur√°cia ({acc_worst_a:.2f}%)")
plt.title("Curvas de Aprendizado ‚Äì ADALINE Multiclasse")
plt.xlabel("√âpocas")
plt.ylabel("Erro Quadr√°tico M√©dio (MSE)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Gerar tabela final com Acur√°cia

modelos = ["ADALINE", "MLP"]
medias = [np.mean(acc_adaline), np.mean(acc_mlp)]
stds = [np.std(acc_adaline), np.std(acc_mlp)]
maximos = [np.max(acc_adaline), np.max(acc_mlp)]
minimos = [np.min(acc_adaline), np.min(acc_mlp)]

print("Modelo     | M√©dia   | Desvio-Padr√£o | Maior Valor | Menor Valor")
print("---------------------------------------------------------------")
for i in range(len(modelos)):
    print(f"{modelos[i]:<10} | {medias[i]:.4f} | {stds[i]:.4f}       | {maximos[i]:.4f}      | {minimos[i]:.4f}")

# Plotar boxplot comparativo
plt.figure(figsize=(6, 4))
plt.boxplot([acc_adaline, acc_mlp], labels=["ADALINE", "MLP"], patch_artist=True)
plt.title("Boxplot de Acur√°cia ‚Äì ADALINE vs MLP")
plt.ylabel("Acur√°cia")
plt.grid(True)
plt.tight_layout()
plt.show()

"""### üìä Boxplot ‚Äì Acur√°cia ADALINE vs. MLP

O MLP apresentou acur√°cia m√©dia ligeiramente superior e melhor desempenho m√°ximo, indicando maior potencial em algumas execu√ß√µes. No entanto, essa vantagem veio acompanhada de **maior dispers√£o nos resultados**, com maior n√∫mero de outliers.

O ADALINE, por outro lado, teve m√©dia um pouco menor, mas mostrou **maior estabilidade**, com menos variabilidade e distribui√ß√£o mais concentrada.

Esses resultados mostram que o MLP pode superar o ADALINE em determinados cen√°rios, mas com menor consist√™ncia ao longo das rodadas.




"""

